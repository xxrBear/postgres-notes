## 性能提示

### 查询计划

在 postgres 中，EXPLAIN 是一个查询分析工具，用于查看一条 SQL 语句的执行计划，帮助你了解查询是如何被数据库理解和执行的，从而进行性能优化

```sql
EXPLAIN SELECT * FROM users WHERE age > 30;
```
> 这条语句不会被执行，而是返回执行计划

```sql
EXPLAIN ANALYZE SELECT * FROM users WHERE age > 30;
```
> 这个会真正执行 SQL 并返回真实的执行时间和耗费资源，更有用

### 规划器使用的统计信息

在 postgres 中，查询规划器会根据统计信息来生成执行计划（比如是否走索引、使用嵌套循环还是哈希连接等）。这些统计信息由 postgres 自动收集，主要反映表的分布、基数、频率等特征。

规划器依赖的统计信息有哪些？

可以通过 `ANALYZE` 命令或自动维护来收集统计信息，它们存储在 `pg_statistic` 和 `pg_stats` 视图中。

表级别统计信息：

| 统计项          | 含义                    |
| --------------- | ----------------------- |
| `reltuples`     | 表中估算的行数          |
| `relpages`      | 表占用的页数            |
| `n_dead_tuples` | 死元组数（Vacuum 使用） |

> 来自系统表 `pg_class`

列级别统计信息：

> 来自系统视图 `pg_stats`

| 字段名              | 含义                                                          |
| ------------------- | ------------------------------------------------------------- |
| `null_frac`         | 该列中 NULL 的比例                                            |
| `n_distinct`        | 该列中唯一值的估算数量                                        |
| `most_common_vals`  | 出现最频繁的值（MCV）                                         |
| `most_common_freqs` | 对应 MCV 的频率                                               |
| `histogram_bounds`  | 值的分布直方图，用于估算范围条件（如 BETWEEN）                |
| `correlation`       | 相关性（值的排序程度），范围 \[-1, 1]，用于决定是否走索引扫描 |
| `avg_width`         | 平均字段宽度（以字节为单位）                                  |

查看统计信息的 SQL 示例

表统计信息：

```sql
SELECT relname, reltuples::bigint, relpages
FROM pg_class
WHERE relname = 'your_table';
```

列统计信息：

```sql
SELECT attname, null_frac, n_distinct, most_common_vals,
       histogram_bounds, correlation
FROM pg_stats
WHERE tablename = 'your_table';
```

手动更新统计信息

```sql
ANALYZE your_table;
```

或者更新整个数据库：

```sql
ANALYZE;
```

对于大数据量导入后，建议执行一次，以帮助规划器做出更优决策

控制统计信息精度

你可以设置字段的统计目标 `n_distinct` 精度：

```sql
ALTER TABLE your_table ALTER COLUMN your_column SET STATISTICS 1000;
ANALYZE your_table;
```

默认是 `100`，设置越大越精确，但会导致收集时间和元数据体积增大

规划器如何用这些信息？

举几个例子：

| 语句                            | 规划器参考什么                                          |
| ------------------------------- | ------------------------------------------------------- |
| `WHERE age = 30`                | `n_distinct` 和 `most_common_vals` 判断有多少行满足条件 |
| `WHERE score BETWEEN 60 AND 90` | `histogram_bounds` 估算选择率                           |
| `ORDER BY column`               | `correlation` 决定是否走 Index Scan                     |
| `JOIN` 操作                     | `n_distinct` 估算基数决定 Hash Join / Merge Join 等     |

`pg_statistic` 原始数据结构

`pg_stats` 是用户友好的视图，而 `pg_statistic` 是底层系统表，存储原始统计信息，比如：

| 字段                                    | 含义                            |
| --------------------------------------- | ------------------------------- |
| `stakind1`, `stavalues1`, `stanumbers1` | 表示第一组统计信息（MCV、频率） |
| `stakind2`, `stavalues2`, `stanumbers2` | 表示第二组（直方图）            |

它以数组形式存储，难以直接解读


| 问题                        | 可能原因                   | 建议                                                     |
| --------------------------- | -------------------------- | -------------------------------------------------------- |
| 执行计划不走索引            | 估算选择率过低             | 执行 `ANALYZE`，检查 `n_distinct` 是否偏低               |
| 走 Seq Scan 而非 Index Scan | `correlation` 为负，表无序 | 尝试强制索引或增加 `random_page_cost`                    |
| JOIN 策略不合理             | 表基数估计偏差大           | 考虑提高统计目标，使用 `pg_stat_statements` 找出问题 SQL |

### 填充数据库

在首次填充数据库时，可能需要插入大量数据。本节包含一些关于如何使此过程尽可能高效的建议。

- 禁用自动提交 

当使用多个 INSERT 时，关闭自动提交并在最后只提交一次。（在普通 SQL 中，这意味着在开始时发出 BEGIN，在结束时发出 COMMIT。某些客户端库可能会在您不知情的情况下执行此操作，在这种情况下，您需要确保该库在您希望它完成时完成。）如果您允许每次插入单独提交，则 postgres 对于添加的每一行都会执行大量工作。在一个事务中执行所有插入的另一个好处是，如果插入一行失败，则会回滚直到该点插入的所有行，因此您不会陷入部分加载的数据。

- 使用 COPY 

使用 COPY 在一个命令中加载所有行，而不是使用一系列 INSERT 命令。COPY 命令针对加载大量行进行了优化；它不如 INSERT 灵活，但对于大型数据加载产生的开销要少得多。由于 COPY 是一个单独的命令，如果您使用此方法填充表，则无需禁用自动提交。

如果您不能使用 COPY，则可以使用 PREPARE 创建预备的 INSERT 语句，然后根据需要多次使用 EXECUTE。这避免了重复解析和规划 INSERT 的一些开销。不同的接口以不同的方式提供此功能；在接口文档中查找 “预备语句”。

请注意，即使使用 PREPARE 并且将多个插入批处理到单个事务中，使用 COPY 加载大量行几乎总是比使用 INSERT 快。

COPY 在与早期的 CREATE TABLE 或 TRUNCATE 命令在同一事务中使用时最快。在这种情况下，不需要写入 WAL，因为如果发生错误，包含新加载数据的文件无论如何都将被删除。但是，此考虑仅适用于 wal_level 为 minimal 的情况，因为所有命令都必须写入 WAL。

- 删除索引 

如果您要加载新创建的表，最快的方法是创建表，使用 COPY 批量加载表的数据，然后创建表所需的任何索引。在预先存在的数据上创建索引比在加载每一行时增量更新它要快。

如果要向现有表中添加大量数据，则删除索引、加载表，然后重新创建索引可能是一个好方法。当然，在索引丢失期间，其他用户的数据库性能可能会受到影响。在删除唯一索引之前也应该三思而后行，因为在索引丢失时，唯一约束提供的错误检查将丢失。

- 删除外键约束 

与索引一样，外键约束可以比逐行更有效地批量检查。因此，删除外键约束、加载数据和重新创建约束可能很有用。同样，在数据加载速度和约束丢失期间的错误检查之间需要权衡。

此外，当您将数据加载到具有现有外键约束的表中时，每个新行都需要在服务器的待处理触发器事件列表中添加一个条目（因为它是触发器的触发来检查该行的外键约束）。加载数百万行可能会导致触发器事件队列溢出可用内存，从而导致无法容忍的交换甚至命令的完全失败。因此，在加载大量数据时，删除和重新应用外键可能 是必要的，而不仅仅是期望的。如果暂时删除约束是不可接受的，那么唯一的其他方法可能是将加载操作拆分为较小的事务。

- 增加 maintenance_work_mem 

在加载大量数据时，临时增加 maintenance_work_mem 配置变量可以提高性能。这将有助于加速 CREATE INDEX 命令和 ALTER TABLE ADD FOREIGN KEY 命令。它对 COPY 本身没有太大作用，因此此建议仅在您使用上述一种或两种技术时才有用。

增加 max_wal_size 

临时增加 max_wal_size 配置变量也可以使大型数据加载更快。这是因为将大量数据加载到 postgres 中将导致检查点的发生频率高于正常检查点频率（由 checkpoint_timeout 配置变量指定）。每当发生检查点时，所有脏页都必须刷新到磁盘。通过在批量数据加载期间临时增加 max_wal_size，可以减少所需的检查点数。

- 禁用 WAL 归档和流复制 

当将大量数据加载到使用 WAL 归档或流复制的安装中时，在加载完成后获取新的基本备份可能比处理大量增量 WAL 数据更快。为了防止在加载时进行增量 WAL 日志记录，请通过将 wal_level 设置为 minimal，将 archive_mode 设置为 off，以及将 max_wal_senders 设置为零来禁用归档和流复制。但是请注意，更改这些设置需要服务器重新启动，并且使之前拍摄的任何基本备份都无法用于归档恢复和备用服务器，这可能会导致数据丢失。

除了避免归档器或 WAL 发送器处理 WAL 数据的时间外，这样做实际上会使某些命令更快，因为如果 wal_level 是 minimal 并且当前子事务（或顶级事务）创建或截断了他们更改的表或索引，它们根本不需要写入 WAL。（通过在末尾执行 fsync 而不是写入 WAL，它们可以更便宜地保证崩溃安全性。）

- 之后运行 ANALYZE 

每当您显著更改了表中数据的分布时，强烈建议运行 ANALYZE。这包括将大量数据批量加载到表中。运行 ANALYZE (或 VACUUM ANALYZE) 可确保规划器拥有关于表的最新统计信息。如果没有统计信息或统计信息过时，规划器在查询规划期间可能会做出错误的决策，导致任何具有不准确或不存在统计信息的表性能不佳。请注意，如果启用了自动清理守护进程，它可能会自动运行 ANALYZE

关于 pg_dump 的一些说明 

pg_dump 生成的转储脚本会自动应用以上几个但不是全部的指导原则。 要尽可能快地还原 pg_dump 转储，您需要手动执行一些额外的操作。（请注意，这些点适用于还原转储时，而不是在创建转储时。 无论是使用 psql 加载文本转储，还是使用 pg_restore 从 pg_dump 归档文件加载，都适用相同的要点。）

默认情况下，pg_dump 使用 COPY，并且在生成完整的模式和数据转储时，它会小心地在创建索引和外键之前加载数据。因此，在这种情况下，一些指导原则是自动处理的。 您需要做的是

为 maintenance_work_mem 和 max_wal_size 设置适当（即比正常情况大）的值。

如果使用 WAL 归档或流复制，请考虑在还原期间禁用它们。为此，请在加载转储之前将 archive_mode 设置为 off，将 wal_level 设置为 minimal，并将 max_wal_senders 设置为零。之后，将其设置回正确的值并进行全新的基本备份。

尝试 pg_dump 和 pg_restore 的并行转储和还原模式，并找到要使用的最佳并发作业数。通过 -j 选项并行转储和还原应该比串行模式提供更高的性能。

考虑是否应将整个转储还原为单个事务。为此，请将 -1 或 --single-transaction 命令行选项传递给 psql 或 pg_restore。 当使用此模式时，即使是最小的错误也会回滚整个还原，可能会丢弃数小时的处理。根据数据的相互关联程度，这可能比手动清理更好，也可能不好。 如果您使用单个事务并且关闭了 WAL 归档，则 COPY 命令将以最快的速度运行。

如果数据库服务器中有多个 CPU 可用，请考虑使用 pg_restore 的 --jobs 选项。这允许并发数据加载和索引创建。

之后运行 ANALYZE。

仅数据转储仍将使用 COPY，但它不会删除或重新创建索引，并且通常不会触及外键。因此，在加载仅数据转储时，如果要使用这些技术，则由您来删除和重新创建索引和外键。 在加载数据时增加 max_wal_size 仍然有用，但不要费心增加 maintenance_work_mem；相反，您将在之后手动重新创建索引和外键时执行此操作。并且不要忘记在完成后运行 ANALYZE

### 非持久化设置

持久性是数据库的一项功能，它保证记录已提交的事务，即使服务器崩溃或断电也是如此。然而，持久性会增加显著的数据库开销，因此，如果您的站点不需要这种保证，可以配置 postgres 以更快地运行。以下是您可以在这种情况下进行的一些配置更改以提高性能。除非下文另有说明，否则在数据库软件崩溃的情况下仍然保证持久性；只有突然的操作系统崩溃才会导致使用这些设置时存在数据丢失或损坏的风险。

- 将数据库集群的数据目录放置在内存支持的文件系统中（即，RAM磁盘）。这消除了所有的数据库磁盘 I/O，但将数据存储限制为可用内存量（以及可能的交换空间）

- 关闭 fsync；无需将数据刷新到磁盘

- 关闭 synchronous_commit；可能无需强制WAL在每次提交时写入磁盘。此设置确实会冒着事务丢失（但不会导致数据损坏）的风险，以防数据库崩溃

- 关闭 full_page_writes；无需防止部分页面写入

- 增加 max_wal_size 和 checkpoint_timeout；这会降低检查点的频率，但会增加 /pg_wal 的存储要求

- 创建未记录的表以避免WAL写入，尽管这会使表无法防崩溃


[返回目录](/README.md)